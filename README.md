# install-llama-cpp
A repository with information on how to get llama-cpp setup with GPU support should you choose.
